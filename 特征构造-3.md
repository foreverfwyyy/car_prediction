# 特征工程-3

问题：

* 为什么对标签做归一化，对训练数据做归一化？

一些想法：

* 长尾分布数据可以去log再归一化

**步骤：**

1. 数据理解

   * 确定数据性质：**定性**数据包括定类和定序，**定量**数据包括定距和定比

2. 数据清洗

   * 缺失值处理：
     - 不处理（针对类似 XGBoost 等树模型）
     - 删除（缺失数据太多）
     - 插值补全，包括均值/中位数/众数/建模预测/多重插补/压缩感知补全/矩阵补全等
     - 分箱，缺失值一个箱

   * 异常处理：
     - 通过箱线图（或 3-Sigma）分析删除异常值
     - BOX-COX 转换（处理有偏分布）
     - 长尾截断

3. 特征构造

   * 构造统计量特征，报告计数、求和、比例、标准差等
   * 时间特征，包括相对时间和绝对时间，节假日，双休日等
   * 地理信息，包括分箱，分布编码等方法
   * 非线性变换（特征归一化/标准化），包括 log/ 平方/ 根号等
     * 标准化（转换为标准正态分布）
     * 归一化（抓换到 [0,1] 区间）
     * 针对幂律分布，可以采用公式： $log(\frac{1+x}{1+median})$
   * 数据分桶
     * 等频分桶
     * 等距分桶
     * Best-KS 分桶（类似利用基尼指数进行二分类）
     * 卡方分桶
   * 特征组合，特征交叉
   * 仁者见仁，智者见智

4. 特征选择（特征筛选）

   * 过滤式（filter）：先对数据进行特征选择，然后在训练学习器，常见的方法有 Relief/方差选择发/相关系数法/卡方检验法/互信息法
   * 包裹式（wrapper）：直接把最终将要使用的学习器的性能作为特征子集的评价准则，常见方法有 LVM（Las Vegas Wrapper） 
   * 嵌入式（embedding）：结合过滤式和包裹式，学习器训练过程中自动进行了特征选择，常见的有 lasso 回归

5. 类别不平衡

若有需要可降维：
- PCA/ LDA/ ICA
- 特征选择也是一种降维



## 本题流程

临时想到的：查看缺失值可以使用可视化函数（没试过）；缺失值可以忽略但要用可以处理缺失值的模型（没试过）；可视化也就是EDA的时候查看倾斜数据：

特征构造分两部分：**第一部分**构造给树模型的特征，**第二部分**接着第一部分继续构造给线性回归和神经网络等模型的特征。

两部分我都进行了改动，感觉很乱，会加一部分自己的理解与改动。

## 构造树模型用特征：

* **删除异常值**：使用箱线图的原理删除某一特征的异常值，箱线图可以查看数据的大致分布，尤其是得到异常值划分点，通过两个四分位数把大部分数据归纳在四分位距中，通过最大最小异常值得到所有异常值。注意：不能删除测试集数据。

* **把训练集和测试集按行拼接**，并新增一列标记是训练集还是测试集，方便特征构造和后续一系列操作。**这里没有采用**，这种做法很容易出问题，数据可能出现问题，数据大小，特征出现错乱，尤其是把数据统一作特征构造，最终预测效果不好，具体的问题暂时还没找到。

* **构造时间特征**：这里是时间差，注意缺失值，可以随时查看空缺值数量及分布。**注意：**构造前时间特征都没有缺失值，构造时间差后出现缺失值，因为原先时间特征有异常值。

* **加入先验知识**：这里是真**不太懂**，*本题是将地区编码（邮编）转成城市特征，而且方法很奇怪，将编码的从头到倒数的第三位的字符串取出作为城市特征，出现缺失值*。**补充**：该特征是邮编信息，前几位表示省或州，后几位表示城市，这里提取前几位，特征更容易表征，我感觉是更具代表性，并且若是提取后几位，城市过多，数据会稀疏。**再次补充：**脱敏前原作者无意得知是德国邮编

* **构造特征统计量**，主要做法是将某一特征分组，然后求统计信息：最大、最小、平均、中位数等其他多个数值，将这些统计值都单独作为一个特征，使用`pandas`左连接到该分组的特征上。注意：使用的是训练数据构造统计信息，且连接完要重设索引，索引这点不太懂。*本题将汽车品牌分组。*

* **数据分桶**：直接使用的`pd.cut`，将数据全部划入几个区间。*本题将power分桶，以下是作者的解释，我**不太懂***，**补充：**其实是``onehot``编码

  ```markdown
  这时候我们的缺失值也进桶了，
  为什么要做数据分桶呢，原因有很多，= =
  1. 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；
  2. 离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰；
  3. LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；
  4. 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；
  5. 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化
  当然还有很多原因，LightGBM 在改进 XGBoost 时就增加了数据分桶，增强了模型的泛化性
  ```

以上分桶原因总结：

<ol>
    <li>离散后内积运算速度快，易存储和扩展。</li>
    <li>特征更鲁棒</li>
    <li>引入非线性，提升模型表达能力，增强拟合</li>
    <li>可以特征交叉，提升表达，引入非线性</li>
    <li>模型更稳定</li>
</ol>

* **删除不需要的数据**：我这里进行了改动，主要是删除了严重倾斜值，倾斜值是在EDA的时候发现的，也采用了一部分本题做法。我认为主要是删除被用来构造别的特征的特征得删除，分桶的没有删除，但是之后还是会再选一次训练特征用哪些。*本题将之前构造时间特征的特征删除了，也删除了构造城市特征的地区编码特征*。
* 保存数据，这里数据没有统一填充缺失值。

## 构造线性模型和神经网络用特征：

接着上一部分的数据继续操作：

* 查看某一特征的分布（直方图），若异常取对数，再进行**归一化**。
* 对数值特征**重复**以上操作，查看分布，然后归一化。具体原因**还不太清楚**。
* 对所有**类别特征进行``onehot``编码**。
* 保存数据。

## 特征选择

主要是做了开头提到的三个方法：过滤式、包裹式、嵌入式。

* **过滤式**：主要是相关性分析，同之前的步骤一样，但不轻易剔除特征，得试了才知道。
* **包裹式：**还不太懂，也没尝试。
* **嵌入式：**主要在模型调参阶段，感觉就是用现成库训练，看哪个效果好，具体也没太懂。

## 总结

感觉这一过程就是把数据清洗一下，然后新增特征并作相应的转换。其中包括异常值处理，我自己也处理了缺失值，还有就是加新特征，时间特征、地理特征（先验知识）、统计特征、分桶和线性模型用的编码及归一化。基本上可以与EDA一起，也会用到图。

## 感悟

自己在做这一块时，很多不懂，就只处理异常值和缺失值，时间、地理和统计特征，操作了分桶，编码做了但没用，归一化实在不懂，效果不好，而且线性模型效果极差，感觉是不会用的原因。所以基本上只用了树模型和其特征，而且特征筛选也没做，也就是所有特征都用在之后的模型训练里了，除了ID类的特征去除了。也没有使用线性模型使用的特征。